from llama_cpp import Llama
import logging
from typing import Optional
import os
from config.settings import AI_ENABLED

logger = logging.getLogger(__name__)

class AIService:
    def __init__(self):
        self.llm = None
        if AI_ENABLED:
            self._load_model()

    def _load_model(self):
        try:
            model_path = "models/Meta-Llama-3-8B-Q8_0.gguf"
            if os.path.exists(model_path):
                logger.info("üîÑ Loading AI model...")
                self.llm = Llama(
                    model_path=model_path,
                    n_ctx=2048,
                    n_threads=4,
                    verbose=False
                )
                logger.info("‚úÖ AI model loaded successfully!")
            else:
                logger.warning("Model file not found, AI will be disabled")
        except Exception as e:
            logger.error(f"‚ùå Model loading error: {e}")

    def generate_response(self, prompt: str) -> Optional[str]:
        if not self.llm:
            return None

        try:
            system_prompt = (
                "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏. "
                "–û—Ç–≤–µ—á–∞–π —á–µ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É (1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è). "
                "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏, –≤–µ–∂–ª–∏–≤–æ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤—å –Ω–∞ —Ç–µ–º—É."
            )
            full_prompt = f"{system_prompt}\n–í–æ–ø—Ä–æ—Å: {prompt}\n–û—Ç–≤–µ—Ç:"

            response = self.llm(
                full_prompt,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
                max_tokens=150,
                temperature=0.6,
                stop=["\n", "–í–æ–ø—Ä–æ—Å:", "–û—Ç–≤–µ—Ç:"],
                echo=False
            )
            return response['choices'][0]['text'].strip()
        except Exception as e:
            logger.error(f"Generation error: {e}")
            return None

ai_service = AIService()