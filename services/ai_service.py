import os
import threading
from pathlib import Path
from llama_cpp import Llama
import logging
from config.settings import AI_ENABLED

logger = logging.getLogger(__name__)

class AIService:
    _lock = threading.Lock()
    
    def __init__(self):
        self.llm = None
        self.model_loaded = False

    def _load_model(self):
        with self._lock:
            if self.llm or self.model_loaded:
                return
                
            try:
                if not AI_ENABLED:
                    logger.info("AI –æ—Ç–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö")
                    return
                
                model_path = "models/saiga_llama3_8b_ggml-model-q8_0.gguf"
                if not Path(model_path).exists():
                    logger.error("–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    return

                logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ AI –º–æ–¥–µ–ª–∏...")
                self.llm = Llama(
                    model_path=model_path,
                    n_ctx=4096,
                    n_threads=8,
                    chat_format="llama-3",
                    n_gpu_layers=50,
                    verbose=False
                )
                self.model_loaded = True
                logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}", exc_info=True)

    def generate_response(self, prompt: str) -> str:
        if not AI_ENABLED or not prompt.strip():
            return ""
            
        if not self.model_loaded:
            self._load_model()
            
        if not self.llm:
            return ""
            
        try:
            system_prompt = (
                "–¢—ã –ö–≤–∞—Ä—Ç–∏—Ä–∫–∞ ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏.\n"
                "–ü–†–ê–í–ò–õ–ê:\n"
                "1. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)\n"
                "2. –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –¥–ª—è –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ üòä\n"
                "3. –ó–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –∫–ª–∏–µ–Ω—Ç–∞\n"
                "4. –ö–∞–∂–¥—ã–µ 3-5 —Ä–µ–ø–ª–∏–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–π –∫–≤–∞—Ä—Ç–∏—Ä—ã\n"
                "5. –ù–∞—á–∏–Ω–∞–π –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã\n\n"
                "–ü—Ä–∏–º–µ—Ä –¥–∏–∞–ª–æ–≥–∞:\n"
                "User: –ü—Ä–∏–≤–µ—Ç\n"
                "You: –ü—Ä–∏–≤–µ—Ç! üòä –ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å –Ω–∞–π—Ç–∏ –∏–¥–µ–∞–ª—å–Ω–æ–µ –∂–∏–ª—å—ë! –ß—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç?"
            )

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            response = self.llm.create_chat_completion(
                messages=messages,
                max_tokens=384,
                temperature=0.7,
                top_p=0.9,
                stop=["<|end_of_text|>"]
            )
            
            content = response['choices'][0]['message']['content'].strip()
            
            # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—É—é –∑–∞–≥–ª–∞–≤–Ω—É—é –±—É–∫–≤—É
            if content and content[0].islower():
                content = content[0].upper() + content[1:]
                
            return content
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}", exc_info=True)
            return ""
